

#58A

- "One outcome of our work is showing how all of the gradual type systems are observationally distinct". That is hardly surprising to anyone.

P: Really? "anyone".. 

- Can you provide a source to the "anecdotal evidence" mentioned on pages 4-5 that blame tracking is helpful to programmers?

P&B: The missing citation here should have been: https://scholarship.rice.edu/handle/1911/18078, where in figure 2.3 of Section 2.3 an example is presented where blame tracking is able to identify which party is at fault for a volation. Further evidences are also presented in [20].

- It would be fair to the readers to point out that gradual typing with blame tracking has *not* gone beyond academic research, even though that observation may not fit well with the paper's strong focus on techniques for blame tracking.

P&B: Blame tracking is outside the scope of this paper. The introduction to section 3 on page 5 explains how blame is intended for future work.


#58B

- The lack of validation against implementations or other formalizations is also problematic.  This is important in general, but especially when monotonic casts are implemented so differently here from other work.

P&B: A validation of the formalisms is necessary, however, implementing the Kafka language is unrealistic, as not every gradual typing systems presented in Kafka are themselves implemented.


Figure 14 shows several different arities for the synthetic cast insertion judgment.  Looking at what appears to the right of the $\Uparrow$, which for lack of a better term I'll refer to as something the judgment "produces":
- MOS1 "produces" a type
- MOS2 "produces" a type and two class tables
- MOS5 "produces" a type, a class table, and a class definition
- MOS2 and MOS3, in the hypotheses that recursively refer to the synthetic judgment, assume it produces a type and one class table

I'm guessing MOS1 should produce a class table as well, and in the other cases the judgment should be read as producing a type and single class table (with some rules concatenating tables in the conclusion, without indicating so explicitly).  Please tell me if I'm mistaken.  This is a switch in judgment form from the other translations, and was not introduced or explained (note that the translation rules for Typed Racket didn't actually appear in the main paper).


P&B: 



The paper contains other passages that seem intended to provide intuition, but actually say things that appear subtly incorrect.  For example, on page 20, the paper remarks that "the monotonic casts only modify the heap if the target C has fewer occurrences of $\star$ than the original type of the object at a," but this suggests that casting an object of type (modulo class names, etc.) $(A,\star)$ to $(\star,A)$ will do nothing instead of refining the type to $(A,A)$.  Siek et al. [21] use a notion of precision to guide refinement, which is concerned with producing more precise types.  I suspect you mean to suggest that the casts will only narrow dynamic types to concrete types, regardless of quantity.






The motivation for modeling monotonic casts by wrapping rather than in-place update was not explained.  On page 20, the paper claims "The monotonic cast semantics in KafKa cannot be implemented using the in-place replacement technique [21]."  Why not?  The paper is not absolutely clear on it, but that paragraph seems to suggest this is because KafKa doesn't permit a field and getter/setter methods of the same name, as method invocation and field access are overloads of the same syntactic construct.  This would be a concern because the casts on field sets are implemented via setter-method wrappers.

This seems like an artificial problem.  On one hand, wouldn't this be better solved by changing KafKa to use more explicitly use something akin to C#'s default implementations of properties (https://msdn.microsoft.com/en-us/library/bb384054.aspx) so the getters/setters can be overridden when an object's class is changed, without changing the rest of the object representation?  This seems like a much simpler than the added complexity of inserting the extra wrappers (which has the extra cost of breaking reference identity when it's not necessary).  On the other hand, why would it be an issue to update the object to replace a physical field with setters/getters?  Field reads and getter invocation are syntactically identical, so it seems like switching the object representation wouldn't cause problems. Or why not extend the dynamic semantics to check for a required cast to perform?  You could surely arrange things such that the check never did anything for objects that had not been monotonically cast.

On its own this choice is not that important, but the paper's goals are to clearly explain the semantics of *existing* approaches.  While this wrapper-based coercion may correctly model the semantics of monotonic casts, having the semantics be destructive as in other work [9,21] would seem to better meet the paper's top-level objectives.  Moreover, monotonic casts preserve object identity in prior work, while here the object identity changes.  This isn't observable in the core language here, but if future work needed that preserved, it couldn't build (directly) on this formalization.

The paper never actually explains the cases where monotonic casts should fail.  Looking at Appendix H, the only case I see the cast getting stuck is if two types that are being joined have incomparable method name sets (i.e., each has a method the other lacks), in which case tmeet will be undefined.  This makes sense for structural meets between recursive types, but is not explained in the paper.  However, in the case that the meet exists but the object being coerced lacks a method present in one of the types (but contains all methods of the other), I have a hard time seeing where that will fail.




- page 10: In the type rule for statically dispatched method calls, wouldn't we prefer to bias the method lookup towards typed method signatures?  Section 4.2 says "typed methods are invoked with static calls and untyped methods are invoked with dynamically resolved calls."  The current type rule would seem to permit static calls to match the type signature of the untyped method as well as the typed method, but it sounds like the dynamic semantics are actually stricter.  If the dynamic semantics don't let static calls dispatch to untyped methods when both are present, then the static dispatch type rule is unsound.

- page 11: I have no idea how to parse this box near the top of the page.  It seems like it's supposed to be a graphically-suggestive presentation of some conditions on the translation, but I don't get it.

- Figure 5 (and others): I'm not sure what the translation of dot to dot is for in TSC2 (other translations have similar rules).  Is this supposed to be translating the hole for an evaluation context?  If so, why is that there, I thought the translations worked on complete programs?

- Figure 9: wrap is invoked with two different arities here: 5 arguments in BC1, 4 arguments in BC2.  Is there a missing argument in BC2?

- The explanation of Figure 12 on page 16 is unclear.  Scrutinizing the figure, it seems like the key change is that *calls* to lifted methods also acquire some casts, rather than just the entry and exit of each method.  But that change isn't spelled out.  And if I'm wrong about the type of change required, then it really needs to be spelled out.  As written, that paragraph simply doesn't explain the details of inserting casts inside the method bodies.

- page 22: The first two sentences on this page (one of which continues from the previous page) seem to contradict each other.







- What is the paper's **key contribution**? The paper did not seem to
  present any new information or particular insights about existing
  systems, only perhaps to express the differences between systems in
  a particularly precise way.

- What set of programs are statically rejected?
  Are there interesting differences to highlight here as well?

- The "Litmus Tests" that distinguish between the various systems felt
  artificial; I would have hoped for tests that would give a good
  feeling for the pros and cons of each approach.

One question for clarification. Kafka offers only structural casts,
but the JVM offers only nominal casts -- and I believe that some
languages, such as Thorn, are also built on nominal casts. Naturally
one can "model" nominal type systems from a structural one (e.g., by
inserting a unique member), but I was surprised not to find any direct
coverage of this in the section on Thorn.

I found the distinction between "gradual Kafka" and "core Kafka" to be
less clear than it could have been; I think partly this is because
there is not one "gradual Kafka" but rather a family of them, as each
"front end" introduces its own concepts (e.g., Thorn has "question"
types like `?C`, and so forth). I would have appreciated a firmer
separation between the two concepts. For example, on page 2, the paper
states:

> Two different structural casts are built-in to Kafka...

then in the same paragraph it says:

> To support some of the more complex type systems, KafKa is extended
> by generative casts which create new wrapper classes.

I think what is happening here is that the "core Kafka" supports two
forms of structural cast, but the "gradual Kafka" supports a richer
set of casts (which are only used in Typed Racket).

Similarly, in the formal sections, I found the underlining sometimes
confusing. For example, On page 13, rule THS3 includes a clause where
the whole thing is underlined:

    __m(t1) : D \in mtypes(C, K)__
    
Rule THS4 includes a very similar clause, but only some parts are underlined:    

    __m__(__t1__) : __?D__ \in mtypes(__C__, __K__)
    
I couldn't tell if this was meant to be conveying some sort of subtle
distinction, or just a stylistic choice. (I sort of suspect that the
formatting in THS3 is just a mistake, since other rules seem to prefer
the THS4-style underlyining.)

* The paper does not give any meta theory for KafKa.
  Is KafKa type safe? Given the novel combination of casts,
  class-based method dispatching, and structural subtyping,
  there is a significant risk that it is not.

* The calculi for the five languages being modeled do not
  seem to capture the important features of those five languages.

* The paper presents five translations, but does not prove
  that the translations faithfully respect the operational semantics
  of the five languages being modeled. The paper does not
  prove that the translations are type preserving.

- The second motivation for the translations is to highlight
  implementation challenges, so it is important for KafKa to
  faithfully represent the JVM and .NET CLR, but the paper does not
  seriously address this issue. Given that KafKa has structural
  subtyping, the difference seems non-trivial. For example, the paper 
  could given a translation from KafKa to the JVM or the CRL.

- The first motivation for the translations is to "capture the essence"
  of five gradually typed languages in a common framework, but
  KafKa has to be extended with different kinds of casts to handle
  the different languages, and the translations for Typed Racket and
  Python are rather convoluted. In particular, the translation for
  Monotonic Python relies on generating proxies, but the point of
  monotonic was to remove the need for proxies.

- Regarding the faithfulness to the five languages, the paper
  presents them all as having bidirectional type
  systems, but many of the languages do not use bidirectional typing.
  Also, an important feature of Typed Racket and Python are
  their first-class classes, but that feature is not modeled here.
