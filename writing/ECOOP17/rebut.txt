We thanks the reviewers for their efforts and while this is not 
the outcome we hoped for, we understand the rationale for your 
reviews and appreciate the constructive criticism.

There is no formal need to read our response, but we would love 
to hear your thoughts as we intend to fix and resubmit.

In particular we will prove type soundness for KafKa, 
implement KafKa on .Net, and remove the monotonic semantics.


                                THANKS!
                    

                 - - - - - - - TL;DR - - - - - - - - -

# 58A

> "One outcome of our work is showing how all of the gradual type systems are
> observationally distinct". That is hardly surprising to anyone.

Paint us as naive but we did not realize how easy it would be to observe
the particulars of an implementation by the runtime errors that we can
generate.

It still surprises us that little attention is paid to trade-off between
what kind of errors can be observed at runtime.  Is this not an interesting
question?

> The title and the introduction of Section 5 are misleading: the section is
> not about translating but about modeling (as it abstracts away from many
> aspects of each of the source languages while focusing on parts related to
> gradual typing that make each language different from the others).

Yes.

> Can you provide a source to the "anecdotal evidence" mentioned on pages 4-5
> that blame tracking is helpful to programmers?

By anecdotal we really mean that: talking to Typed Racket users in the
office next door.

There is also https://scholarship.rice.edu/handle/1911/18078, where in
figure 2.3 of Section 2.3 an example is presented where blame tracking is
able to identify which party is at fault for a violation. See as well [20].

No further user study has been done on blame.


> "Gradual typing is no longer simply a popular research topic in academia"
> ... That certainly depends on
> whether you mean "gradual typing" with or without blame tracking ... In the
> former case, the statement is incorrect ... It would
> be fair to the readers to point out that gradual typing with blame tracking
> has not gone beyond academic research, even though that observation may not
> fit well with the paper's strong focus on techniques for blame tracking.

A terminology issue may exist here. We explicitly ignore blame tracking, 
as we think that the detail of error messages is orthogonal to what causes
the errors in the first place.

[Blame tracking refers to the ability to pinpoint which boundary crossing
 cause an error. It is used in Typed Racket and has a small number of
 commercial users.]
 

# 58B

> The presentation of monotonic casts was not very clear Much of the technical
> machinery is treated inconsistently, adding confusion The generative casts
> share little in common with the lightweight casts, making the value of
> having them reside in a common core language (as opposed to a common
> framework) harder to see The monotonic casts are modeled in a very different
> way from prior work (wrapping instead of updating), which seems to
> complicate the model but isn't well-motivated

Sadly monotonic has had us twisting into pretzels to try to accommodate its
baroque semantics.  

We adopt the one-level wrapper scheme with in-place wrapper replacement as it 
provides the same fixed cost access property, while remaining within the 
semantics of Kafka (up to the addition of the monotonic cast operator).

However, due to the current state of the monotonic implementation in 
Reticulated Python (which stopped working recently), we are considering 
dropping it from our study.

Thoughts?


> The paper takes the right general approach to contrasting the semantics, and
> is indeed the first paper to lay out the semantics of all of these systems
> in a common formulation --- this adds quite a bit of clarity without the
> requirement of so much study of the formulated systems. The relationship
> between the systems is spelled out quite clearly (aside from my concerns
> about the generative casts), and even identifying the placement of casts by
> each approach in the translation to KafKa is very useful.

Thanks.

> My biggest concern with the paper is that the treatment of monotonic casts
> appears to be poorly explained and deviates in major ways from prior
> implementations [9,21] without good reason. The explanation is sufficiently
> poor that I needed to stare at tmeet and the appendix for a long time to
> figure out how it was possible for a monotonic cast to fail in the
> semantics.

:-(  We can only shake our heads in agreement.  And feel sympathy.
Thanks for slogging through it.


> The lack of validation against implementations or other formalizations is
> also problematic. This is important in general, but especially when
> monotonic casts are implemented so differently here from other work.

We wondered how to do that many times. Monotonic does not have a formalization
in terms of an object calculus -- there are some issues that we had to
reverse engineer from the implementation. But that implementation was
far from stable. 


> Figure 14 shows several different arities for the synthetic cast insertion
> judgment. Looking at what appears to the right of the ⇑, which for lack of a
> better term I'll refer to as something the judgment "produces":
> MOS1 "produces" a type MOS2 "produces" a type and two class tables MOS5
> "produces" a type, a class table, and a class definition MOS2 and MOS3, in
> the hypotheses that recursively refer to the synthetic judgment, assume it
> produces a type and one class table
> I'm guessing MOS1 should produce a class table as well, and in the other
> cases the judgment should be read as producing a type and single class table
> (with some rules concatenating tables in the conclusion, without indicating
> so explicitly). Please tell me if I'm mistaken. This is a switch in judgment
> form from the other translations, and was not introduced or explained

The MOSx rules are consistent, producing a list of classes that will be
added to the global class table. For MOS1, the list of classes is omitted as
MOS1 produces an empty list of classes. This should have be clarified by
adding explanatory text for MOS1.


> The paper contains other passages that seem intended to provide intuition,
> but actually say things that appear subtly incorrect. For example, on page
> 20, the paper remarks that "the monotonic casts only modify the heap if the
> target C has fewer occurrences of ⋆ than the original type of the object at
> a," but this suggests that casting an object of type (modulo class names,
> etc.) ... Siek et al. [21] use a notion of precision to guide refinement, which
> is concerned with producing more precise types. I suspect you mean to
> suggest that the casts will only narrow dynamic types to concrete types,
> regardless of quantity.

You are correct, and our formalism captures the same narrowing property
that the original monotonic formalization did. Thank you for pointing 
out the imprecision.

> Why not in-place updates for monotonic casts?

Monotonic casts are only sound with in-place replacement if the language's type
system allows consistency, and Kafka's (along with most other intermediate
languages, like Java bytecode and CIL) doesn't. As a result, we have to use
untyped getter and setters instead, which lead into more issues, as discussed
next.

> The motivation for modeling monotonic casts by wrapping rather than in-place
> update was not explained. On page 20, the paper claims "The monotonic cast
> semantics in KafKa cannot be implemented using the in-place replacement
> technique [21]." Why not? The paper is not absolutely clear on it, but that
> paragraph seems to suggest this is because KafKa doesn't permit a field and
> getter/setter methods of the same name, as method invocation and field
> access are overloads of the same syntactic construct. This would be a
> concern because the casts on field sets are implemented via setter-method
> wrappers.


We wanted to have direct field access (without forcing everything to go
through method calls).  In order to guard field assignment under in-place
replacement, a setter method is required to enforce the monotonic
invariant. However, in Kafka a semantic ambiguity would occur when both a
field declaration and a getter-setter combination exist for the same field
in the same class. To avoid this, we lift the getter-setter out of the object
and into a wrapper, which is then updated by the monotonic cast.s


> This seems like an artificial problem. On one hand, wouldn't this be better
> solved by changing KafKa to use more explicitly use something akin to C#'s
> default implementations of properties
> (https://msdn.microsoft.com/en-us/library/bb384054.aspx) so the
> getters/setters can be overridden when an object's class is changed, without
> changing the rest of the object representation? This seems like a much
> simpler than the added complexity of inserting the extra wrappers (which has
> the extra cost of breaking reference identity when it's not necessary). On
> the other hand, why would it be an issue to update the object to replace a
> physical field with setters/getters? Field reads and getter invocation are
> syntactically identical, so it seems like switching the object
> representation wouldn't cause problems. Or why not extend the dynamic
> semantics to check for a required cast to perform? You could surely arrange
> things such that the check never did anything for objects that had not been
> monotonically cast.

This would introduce additional overhead to every the other gradual typing
systems, even when they do not require this behavior. One of the design
decision for Kafka is to maximum the ability to compare the systems against
each other, while minimizing the amount of unnecessary features that are not
part of the essential aspect of their respective original system.

What we realized writing this paper is that monotonic is unimplementable in
a stock VM. And, more likely than not, an unworkable idea.


Another issue is it would break KafKa's notion of subtyping, which does not
allow for consistency. For example, consider the following, where t =/= t'
and where t meet t' =/= t and t':

class C:
  f:t
class D:
  f:t'

c = new C(...)
d = <D>c

Here, if t'' = t meet t', we would generate a class of the form

class CtoD:
  f':t
  f(x:t''):t''
    this.f' = <t>x
  f():t''
    <t''>this.f'

However, note that C has type {f(t):t,f():t}, whereas CtoD has type
{f(t''):t'',f():t''}. Since t'' is not a subtype of t, it follows that CtoD
is not a subtype of C, and therefore we cannot substitute a class of CtoD
for C while keeping the heap well-typed with respect to the KafKa typing
relation.



> On its own this choice is not that important, but the paper's goals are to
> clearly explain the semantics of existing approaches. While this
> wrapper-based coercion may correctly model the semantics of monotonic casts,
> having the semantics be destructive as in other work [9,21] would seem to
> better meet the paper's top-level objectives. Moreover, monotonic casts
> preserve object identity in prior work, while here the object identity
> changes. This isn't observable in the core language here, but if future work
> needed that preserved, it couldn't build (directly) on this formalization.

Thanks to a technical detail, Kafka's monotonic semantics
does retain object identity, as each object is forever associated with exactly
one wrapper, whose identity is equivalent to the original object's (and which
is updated by the monotonic cast). However, this is virtually by accident.

A mark of a good formalism is it's elegance. There is some discussion to be
made whether the ugliness of monotonic is inherent.

> The paper never actually explains the cases where monotonic casts should
> fail. Looking at Appendix H, the only case I see the cast getting stuck is
> if two types that are being joined have incomparable method name sets (i.e.,
> each has a method the other lacks), in which case tmeet will be
> undefined. This makes sense for structural meets between recursive types,
> but is not explained in the paper. However, in the case that the meet exists
> but the object being coerced lacks a method present in one of the types (but
> contains all methods of the other), I have a hard time seeing where that
> will fail.

This failure is caught in the rule CMS1 in section H.2 of the appendix at
page 35. CMS1 ensures the methods in the casting type is a subset of the
methods in the type of the expression being casted on.


> page 10: In the type rule for statically dispatched method calls, wouldn't
> we prefer to bias the method lookup towards typed method signatures? Section
> 4.2 says "typed methods are invoked with static calls and untyped methods
> are invoked with dynamically resolved calls." The current type rule would
> seem to permit static calls to match the type signature of the untyped
> method as well as the typed method, but it sounds like the dynamic semantics
> are actually stricter. If the dynamic semantics don't let static calls
> dispatch to untyped methods when both are present, then the static dispatch
> type rule is unsound.

Static method calls are matched with their type signature to the respective
typed or untyped methods. There is inconsistency between the static method
typing rules shown in page 10, and the dynamic semantics shown at the bottom
of page 11. The subscript of t -> t' in m is used to match the method call
to the respective typed method, when t and t' are C, or untyped method, when
t and t' are star.

> page 11: I have no idea how to parse this box near the top of the page. It
> seems like it's supposed to be a graphically-suggestive presentation of some
> conditions on the translation, but I don't get it.

Sorry we will clear up.


> Figure 4: These rules could really use parentheses around the compound
> expressions, particularly the casts, to make the figure easier to parse.

OK.

> Figure 4 / Section 4: The behavioral and monotone casts are omitted from the
> initial formal presentation, and added later. You should mention this
> explicitly in Section 4.

Yes.

> Figure 5 (and others): I'm not sure what the translation of dot to dot is
> for in TSC2 (other translations have similar rules). Is this supposed to be
> translating the hole for an evaluation context? If so, why is that there, I
> thought the translations worked on complete programs?

This is a mistake, translation does work on complete programs. TSC2 is the
translation for classes when the class table is empty, the class definition
in front of the class table is omitted by mistake.


> Section 5.4: This first sentence about the "macro model" makes no sense to
> me, as I'm not an expert on Typed Racket. It could use a brief explanation.

Ok.

> Figure 9: wrap is invoked with two different arities here: 5 arguments in
> BC1, 4 arguments in BC2. Is there a missing argument in BC2?

No, BC1 is the wrapper function for non-star type , while BC2 is the wrapper
function for the star type.

> Figure 10: I think Figure 10 is typeset immediately below an in-line example
> of wrap. But visually it looks like both wrap examples are part of the
> figure, making it hard to interpret the reference to Figure 10 in the
> "Copying the behaviour..." paragraph. I know this is partly due to luck of
> the layout algorithm, but consider playing with the optional placement
> arguments to the figure environment.

OK.

> The explanation of Figure 12 on page 16 is unclear. Scrutinizing the figure,
> it seems like the key change is that calls to lifted methods also acquire
> some casts, rather than just the entry and exit of each method. But that
> change isn't spelled out. And if I'm wrong about the type of change
> required, then it really needs to be spelled out. As written, that paragraph
> simply doesn't explain the details of inserting casts inside the method
> bodies.

You are not wrong, the body of the lifted methods require additional casts
as well as have the variable "this" re-binded with the correct type.

> page 22: The first two sentences on this page (one of which continues from
> the previous page) seem to contradict each other.

Wrappers are overly conservative as they treat all non-star fields as star
fields, the reason for the two cast inside the method body of the wrappers.
If the type of the fields becomes specialized, subtyping would no longer
hold.

# 58C

> My main concern with the paper as-is is that it does not provide a result on
> which the community could gain confidence in these definitions. There are
> many possible ways to do this, but the paper needs to provide something
> along these lines before I could advocate for it.

The challenge is that we don't really have reference semantics for any
of the calculi and the implementations are very different.

- Thorn's semantics is by translation.

- Typed Racket is layered on Racket and on contracts, classes are macros and objects,
and unentangling them is nontrivial.

- Reticulated Python has no semantics, there is a semantics for a non-object
functional version.

> One baseline validation would be an implementation of KafKa and compilers
> for the source languages into KafKa. This could be as minimal as a simple
> interprefer for KafKa--it need not be performant, along with compilers for
> reasonable subsets of the source language. This would enable at least
> conformance testing the translations by comparing executions of programs
> compiled to KafKa with those with their native execution environments.

That is a good idea -- we will follow it for our next iteration.

> Such an implementation could also enable providing a reference execution
> environment for comparing the performance of difference decision choices in
> gradually-typed language design. This could be an interesting way to isolate
> the performance impact of the language design from the differences in
> engineering present between the different execution environments.

Not sure that we will be to talk about performance. Even if we could show
that a naive interpreter for, say, Thorn is faster than Typed Racket,
the threat to validity would be that we are not implementing optimizations...

Still, an implementation would be nice.

> Section 3.6 also hints at something potentially very interesting: designing
> a suite of litmus tests that could exhibit differences in gradually-typed
> language design.

We also believe the same.

> # 58D

> What is the paper's key contribution? The paper did not seem to present any
> new information or particular insights about existing systems, only perhaps
> to express the differences between systems in a particularly precise way.

One thing that we did not really explain is that the design of KafKa was meant
to match the features provided by a VM like the JVM or .Net. So, these calculi
should be mapped easily on to those system.

Sadly Monotonic breaks that goal -- as the monotonic cast is not easily implemented
without extensive VM support.


> The calculus helped to clarify the dynamic semantics of the various gradual
> typing systems, but I found it less helpful in understanding their static
> semantics.

Understood.

> In other words, what set of programs are statically rejected? Are there
> interesting differences to highlight here as well?

The static semantics feel less surprising to us, and we wanted to highlight
the dynamic differences between the systems. As a result, we tried to normalize
the statics to the greatest extent possible, though the following systems 
vary:

- Transient & Monotonic semantics have consistency
- Thorn has weak/optional types

> The "Litmus Tests" that distinguish between the various systems felt
> artificial; I would have hoped for tests that would give a good feeling for
> the pros and cons of each approach.

The point was to show that each design will reject different programs
at runtime. We felt it interesting that it was easy to show the
difference on simple programs.


> One thing that the introduction of the paper makes clear is that the term
> "gradual typing" can be used to mean many different things. The promise of
> the paper then is that the Kafka framework can be used to effectively
> compare these distinct approaches, and I think that to some extent it
> succeeds.

Thanks -- we agree that monotonic is the wart in the ointment. 

> However, I also had the feeling that the Kafka formalization itself seemed
> to add relatively little atop a more general comparison. Moreover, perhaps
> because the systems vary from one another, I had the impression that the
> features used by the various systems did not overlap so much with one
> another.

Perhaps this, in itself, is interesting.

> One question for clarification. Kafka offers only structural casts, but the
> JVM offers only nominal casts -- and I believe that some languages, such as
> Thorn, are also built on nominal casts. Naturally one can "model" nominal
> type systems from a structural one (e.g., by inserting a unique member), but
> I was surprised not to find any direct coverage of this in the section on
> Thorn.

For Thorn and Typed Racket the choice between structural and nominal is somewhat
orthogonal to how gradual typing works.  For Monotonic, structural is crucial
as the type of an object is updated by casts.  (Nominal would simply not
work).

We could have added nominal but it did not feel particularly crucial to the
development.


> I found the distinction between "gradual Kafka" and "core Kafka" to be less
> clear than it could have been; I think partly this is because there is not
> one "gradual Kafka" but rather a family of them, as each "front end"
> introduces its own concepts (e.g., Thorn has "question" types like ?C, and
> so forth). I would have appreciated a firmer separation between the two
> concepts. For example, on page 2, the paper states:
> Two different structural casts are built-in to Kafka...
> then in the same paragraph it says:
> To support some of the more complex type systems, KafKa is extended by
> generative casts which create new wrapper classes.
> I think what is happening here is that the "core Kafka" supports two forms
> of structural cast, but the "gradual Kafka" supports a richer set of casts
> (which are only used in Typed Racket).

Yes. This should be clearer.

> Similarly, in the formal sections, I found the underlining sometimes
> confusing. For example, On page 13, rule THS3 includes a clause where the
> whole thing is underlined:

We will clear up.

> # 58E

> The paper does not give any meta theory for KafKa. Is KafKa type safe? Given
> the novel combination of casts, class-based method dispatching, and
> structural subtyping, there is a significant risk that it is not.

We will add proof of soundness for KafKa in our next go-around.


> The paper presents five translations, but does not prove that the
> translations faithfully respect the operational semantics of the five
> languages being modeled. The paper does not prove that the translations are
> type preserving.

> The paper does not present empirical results of any kind. It does not
> present performance results or case studies, etc.

True.  

> The second motivation for the translations is to highlight implementation
> challenges, so it is important for KafKa to faithfully represent the JVM and
> .NET CLR, but the paper does not seriously address this issue. Given that
> KafKa has structural subtyping, the difference seems non-trivial. For
> example, the paper could given a translation from KafKa to the JVM or the
> CRL.

Yes, this is an idea we will pursue.


> The first motivation for the translations is to "capture the essence" of
> five gradually typed languages in a common framework, but KafKa has to be
> extended with different kinds of casts to handle the different languages,
> and the translations for Typed Racket and Python are rather convoluted. In
> particular, the translation for Monotonic Python relies on generating
> proxies, but the point of monotonic was to remove the need for proxies.

Ah monotonic. See above.

> Regarding the faithfulness to the five languages, the paper presents them
> all as having bidirectional type systems, but many of the languages do not
> use bidirectional typing. Also, an important feature of Typed Racket and
> Python are their first-class classes, but that feature is not modeled here.

First class classes are orthogonal to the points discussed in the paper.

Or?

> "The monotone cast <|t|> a returns a wrapper ..." Hmm, the point of
> monotonic references was to avoid wrappers.

Please see above.

> "casts are inserted on method entry and prior to returning" Transient does
> not insert casts prior to returning, but instead inserts a cast around the
> call to the method.

Ok.

> "The translation to KafKa involves creating a single wrapper that encodes
> the  type of the object. Monotonic generative casts are inserted,
> where appropriate, to create wrapper classes that further specialize and
> enforce the effective type, replacing the original, more dynamic, wrapper."

> Unclear whether this is faithful to monotonic, certainly not from a
> performance perspective .

We yield on monotonic. 

> Why bi-directional typing?


The use of bidirectional typing is forced by the combination of 
an explicit cast operator alongside subsumption. If the two typing rules:


Gamma |- e : t'
---------------- (Cast)
Gamma |- <t> e : t


Gamma |- e : t'   t' <: t
------------------------- (Sub)
Gamma |- e : t

are combined, then the instantiation for t' becomes non-deterministic, and 
the results of type-driven translation become very odd indeed.

> "As a result of generating the wrappers (and the static translation), the
> simple example shown in figure 15 does not correctly exhibit the correct
> behavior of the monotonic semantics in KafKa"

> That's bad!

Monotonic! We present the fix for this later, but it drives much of the 
massive complexity of the monotonic system. While we can clarify this,
we will likely not consider monotonic for future versions of this paper.
